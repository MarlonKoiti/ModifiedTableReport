{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5938e9-0cb1-435e-a848-6d002f932581",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f2efd6-a92e-44d4-a891-2a4f1ed5bcc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import DateType, IntegerType, StringType, DoubleType\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27747b77-3ea2-48fc-b2e7-650172173839",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d623aa9c-306f-4e73-a1cb-69a9e1cef20a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Inserir yours databases\n",
    "lista_database = [''] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12cf3afe-fa49-45f4-9284-14dfdf713d61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Report data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b9a869-4ee4-4a3f-a6cc-b5c61f4ef90f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Declaration of dataframes\n",
    "df_detalhes = 0\n",
    "df_detalhes_appended = pd.DataFrame()\n",
    "data_atual = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "#Start of the Loop to see the databases\n",
    "for db in lista_database_talend:\n",
    "  df_metadata = spark.sql('SHOW TABLES IN  '+db)\n",
    "  df_pandas_md  = df_metadata.select('database', 'tableName').toPandas()\n",
    "\n",
    "#list of Schemas and tables inside pandas dataframe\n",
    "  for database,table in df_pandas_md.values:\n",
    "    if not isinstance(df_detalhes, pyspark.sql.dataframe.DataFrame):\n",
    "      try:  #First pass to each database  \n",
    "        df_detalhes = spark.sql(' DESCRIBE DETAIL '+database+'.'+table)\n",
    "        print('1º Ok /', database+'.'+table, ' / criação')\n",
    "      except:\n",
    "         print('1º Error', database+'.'+table)\n",
    "    else:\n",
    "      try:  #Second pass for each database  \n",
    "        df_detalhes = df_detalhes.union(spark.sql(' DESCRIBE DETAIL '+database+'.'+table))\n",
    "        print('2º Ok /', database+'.'+table, ' / union')\n",
    "      except:\n",
    "        print('2º Error', database, table)\n",
    "df_detalhes = df_detalhes.withColumn(\"dataExec\", lit(data_atual).cast(StringType()))\n",
    "df_detalhes = df_detalhes.toPandas()\n",
    "df_detalhes_appended = df_detalhes_appended.append(df_detalhes)\n",
    "  \n",
    "df_final = spark.createDataFrame(df_detalhes_appended[['name','createdAt','lastModified', 'sizeInBytes', 'dataExec']].sort_values('lastModified', ascending=False)).toPandas()\n",
    "print(\"End of loads.\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f207f7db-6252-4b4b-a3ce-599ec50dd2b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad91a911-616b-4231-8586-66072e2f7f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creates the dataframe with the final result of data generation\n",
    "#create a temporary table to do the comparison\n",
    "df_final = spark.createDataFrame(df_final)\n",
    "df_final.createOrReplaceTempView(\"ctrl_carga_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a14e8f4-e673-40cb-acbd-db6c183e3357",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5c09f5-0820-41ac-ac25-62e858541a22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Create a failure table based on the data that was inserted into the failed temp table\n",
    "create or replace table stage.ctrl_carga_falhas as\n",
    "  select org.* \n",
    "  from ctrl_carga_temp tmp\n",
    "  inner join stage.ctrl_carga_falhas org\n",
    "    on org.name = tmp.name \n",
    "  where org.lastModified >= tmp.lastModified\n",
    "  and org.sizeInBytes >= tmp.sizeInBytes\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9874fdfb-4238-44e4-8878-740503a55212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#inserts the data collected by the report into the physical table\n",
    "permanent_table_name = \"stage.ctrl_carga_falhas\"\n",
    "df_final.write.mode('append').format(\"delta\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105f7e3e-5b68-4540-a700-21cbab1c20ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Delete data from table with execution date less than current day - 2\n",
    "DELETE FROM stage.ctrl_carga_falhas\n",
    "WHERE date_format(to_date(dataExec, 'dd/MM/yyyy'), 'yyyy-MM-dd') <= date_sub(current_date, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4704dde-390c-47c2-8289-0ac955bf67bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Delete data from table with execution date less than current day - 1\n",
    "DELETE FROM stage.ctrl_carga\n",
    "WHERE date_format(to_date(dataExec, 'dd/MM/yyyy'), 'yyyy-MM-dd') <= date_sub(current_date, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22fb84ff-a88e-4861-bb48-deea53c0708e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Report extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da113ee-98af-4d2f-8d1c-01f81c8ea7ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#crash report\n",
    "df_falha = spark.read.table('stage.ctrl_carga_falhas')\n",
    "df_falha.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38614f59-74e8-4011-9d33-e8179af5e736",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#data loads report\n",
    "df_table = spark.read.table('stage.ctrl_carga')\n",
    "df_table.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1662595354750100,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "RelatórioCargasDatabricks",
   "notebookOrigID": 2470808106854332,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
